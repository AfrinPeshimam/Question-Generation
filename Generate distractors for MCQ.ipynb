{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Text : The Nile River\n",
    "\n",
    "The Greek historian knew what he was talking about. The Nile River fed Egyptian civilization for hundreds of years.\n",
    "\n",
    "The Longest River, The Nile is 4,160 miles long - the world’s longest river. It begins near the equator in Africa and flows north to the Mediterranean Sea. \n",
    "\n",
    "In the south it churns with cataracts. A cataract is a waterfall. Near the sea the Nile branches into a delta. A delta is an area near a river’s mouth where the water deposits fine soil called silt. In the delta, the Nile divides into many streams. \n",
    "\n",
    "The river is called the upper Nile in the south and the lower Nile in the north. For centuries, heavy rains in Ethiopia caused the Nile to flood every summer. The floods deposited rich soil along the Nile’s shores. This soil was fertile, which means it was good for growing crops. Unlike the Tigris and Euphrates, the Nile River flooded at the same time every year, so farmers could predict when to plant their crops.\n",
    "\n",
    "\n",
    "## Question : Which is the world's longest river ?\n",
    "a) ____________________________________\n",
    "\n",
    "b) Nile \n",
    "\n",
    "c) ____________________________________\n",
    "\n",
    "d) ____________________________________\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Nile -------------------->  Distractor generation algorithms ------> Missisipi, Amazon , Yangtze\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using Wordnet to generate distractors (Wrong Choices) \n",
    "\n",
    "https://wordnet.princeton.edu/citing-wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.5.0 in c:\\anaconda\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: regex in c:\\anaconda\\lib\\site-packages (from nltk==3.5.0) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\lib\\site-packages (from nltk==3.5.0) (4.64.0)\n",
      "Requirement already satisfied: joblib in c:\\anaconda\\lib\\site-packages (from nltk==3.5.0) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\anaconda\\lib\\site-packages (from nltk==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from click->nltk==3.5.0) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Afrin\n",
      "[nltk_data]     Peshimam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bat.n.01') :  nocturnal mouselike mammal with forelimbs modified to form membranous wings and anatomical adaptations for echolocation by which they navigate \n",
      "\n",
      "Synset('bat.n.02') :  (baseball) a turn trying to get a hit \n",
      "\n",
      "Synset('squash_racket.n.01') :  a small racket with a long handle used for playing squash \n",
      "\n",
      "Synset('cricket_bat.n.01') :  the club used in playing cricket \n",
      "\n",
      "Synset('bat.n.05') :  a club used for hitting a ball in various games \n",
      "\n",
      "Synset('bat.v.01') :  strike with, or as if with a baseball bat \n",
      "\n",
      "Synset('bat.v.02') :  wink briefly \n",
      "\n",
      "Synset('bat.v.03') :  have a turn at bat \n",
      "\n",
      "Synset('bat.v.04') :  use a bat \n",
      "\n",
      "Synset('cream.v.02') :  beat thoroughly and conclusively in a competition or fight \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SAME WORD CAN BE USED IN DIFFERENT CONTEXT\n",
    "\n",
    "word = \"bat\"\n",
    "word = word.lower()\n",
    "syns = wn.synsets(word)\n",
    "\n",
    "for syn in syns:\n",
    "  print (syn, \": \",syn.definition(),\"\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bat.n.01') :  nocturnal mouselike mammal with forelimbs modified to form membranous wings and anatomical adaptations for echolocation by which they navigate \n",
      "\n",
      "Synset('bat.n.02') :  (baseball) a turn trying to get a hit \n",
      "\n",
      "Synset('squash_racket.n.01') :  a small racket with a long handle used for playing squash \n",
      "\n",
      "Synset('cricket_bat.n.01') :  the club used in playing cricket \n",
      "\n",
      "Synset('bat.n.05') :  a club used for hitting a ball in various games \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get only noun synsets\n",
    "\n",
    "\n",
    "# Question : Which of these is a nocturnal animal that flies?\n",
    "# a) _________\n",
    "# b) _________\n",
    "# c) bat\n",
    "# d) _________\n",
    "\n",
    "\n",
    "word = \"bat\"\n",
    "word = word.lower()\n",
    "syns = wn.synsets(word,'n')\n",
    "\n",
    "for syn in syns:\n",
    "  print (syn, \": \",syn.definition(),\"\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('big_cat.n.01')]\n",
      "[Synset('cheetah.n.01'), Synset('jaguar.n.01'), Synset('leopard.n.02'), Synset('liger.n.01'), Synset('lion.n.01'), Synset('saber-toothed_tiger.n.01'), Synset('snow_leopard.n.01'), Synset('tiger.n.02'), Synset('tiglon.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# Get hypernyms for a synset\n",
    "\n",
    "word = \"lion\"\n",
    "word = word.lower()\n",
    "syns = wn.synsets(word,'n')\n",
    "\n",
    "\n",
    "hypernym = syns[0].hypernyms()\n",
    "print (hypernym)\n",
    "print (hypernym[0].hyponyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original word:  Lion\n",
      "['Cheetah', 'Jaguar', 'Leopard', 'Liger', 'Saber-toothed Tiger', 'Snow Leopard', 'Tiger', 'Tiglon']\n",
      "\n",
      "original word:  Bat\n",
      "['Aardvark', 'Aquatic Mammal', 'Buck', 'Bull', 'Carnivore', 'Cow', 'Digitigrade Mammal', 'Doe', 'Edentate', 'Fissipedia', 'Flying Lemur', 'Hyrax', 'Insectivore', 'Lagomorph', 'Livestock', 'Pachyderm', 'Pangolin', 'Plantigrade Mammal', 'Primate', 'Proboscidean', 'Rodent', 'Tree Shrew', 'Unguiculata', 'Unguiculate', 'Ungulata', 'Ungulate', 'Yearling']\n",
      "\n",
      "original word:  Green\n",
      "['Blond', 'Blue', 'Brown', 'Complementary Color', 'Olive', 'Orange', 'Pastel', 'Pink', 'Purple', 'Red', 'Salmon', 'Yellow']\n"
     ]
    }
   ],
   "source": [
    "# Distractors from Wordnet\n",
    "def get_distractors_wordnet(syn,word):\n",
    "    distractors=[]\n",
    "    word= word.lower()\n",
    "    orig_word = word\n",
    "    if len(word.split())>0:\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    hypernym = syn.hypernyms()\n",
    "    if len(hypernym) == 0: \n",
    "        return distractors\n",
    "    for item in hypernym[0].hyponyms():\n",
    "        name = item.lemmas()[0].name()\n",
    "        #print (\"name \",name, \" word\",orig_word)\n",
    "        if name == orig_word:\n",
    "            continue\n",
    "        name = name.replace(\"_\",\" \")\n",
    "        name = \" \".join(w.capitalize() for w in name.split())\n",
    "        if name is not None and name not in distractors:\n",
    "            distractors.append(name)\n",
    "    return distractors\n",
    "\n",
    "\n",
    "original_word = \"lion\"\n",
    "synset_to_use = wn.synsets(original_word,'n')[0]\n",
    "distractors_calculated = get_distractors_wordnet(synset_to_use,original_word)\n",
    "\n",
    "print (\"original word: \",original_word.capitalize())\n",
    "print (distractors_calculated)\n",
    "\n",
    "\n",
    "original_word = \"bat\"\n",
    "synset_to_use = wn.synsets(original_word,'n')[0]\n",
    "distractors_calculated = get_distractors_wordnet(synset_to_use,original_word)\n",
    "\n",
    "print (\"\\noriginal word: \",original_word.capitalize())\n",
    "print (distractors_calculated)\n",
    "\n",
    "original_word = \"green\"\n",
    "synset_to_use = wn.synsets(original_word,'n')[0]\n",
    "distractors_calculated = get_distractors_wordnet(synset_to_use,original_word)\n",
    "\n",
    "print (\"\\noriginal word: \",original_word.capitalize())\n",
    "print (distractors_calculated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('cricket.n.01') :  leaping insect; male makes chirping noises by rubbing the forewings together \n",
      "\n",
      "Synset('cricket.n.02') :  a game played with a ball and bat by two teams of 11 players; teams take turns trying to score runs \n",
      "\n",
      "\n",
      "original word:  Cricket\n",
      "['Grasshopper']\n",
      "\n",
      "original word:  Cricket\n",
      "['Ball Game', 'Field Hockey', 'Football', 'Hurling', 'Lacrosse', 'Polo', 'Pushball', 'Ultimate Frisbee']\n"
     ]
    }
   ],
   "source": [
    "#  An example of a word with two different senses\n",
    "original_word = \"cricket\"\n",
    "\n",
    "syns = wn.synsets(original_word,'n')\n",
    "\n",
    "for syn in syns:\n",
    "  print (syn, \": \",syn.definition(),\"\\n\" )\n",
    "\n",
    "\n",
    "synset_to_use = wn.synsets(original_word,'n')[0]\n",
    "distractors_calculated = get_distractors_wordnet(synset_to_use,original_word)\n",
    "\n",
    "print (\"\\noriginal word: \",original_word.capitalize())\n",
    "print (distractors_calculated)\n",
    "\n",
    "\n",
    "original_word = \"cricket\"\n",
    "synset_to_use = wn.synsets(original_word,'n')[1]\n",
    "distractors_calculated = get_distractors_wordnet(synset_to_use,original_word)\n",
    "\n",
    "print (\"\\noriginal word: \",original_word.capitalize())\n",
    "print (distractors_calculated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Conceptnet to generate distractors\n",
    "\n",
    "https://conceptnet.io/\n",
    "\n",
    "Question: Arnold Schwarzenegger served as a governor to which state?\n",
    "\n",
    "a) _________\n",
    "\n",
    "b) California\n",
    "\n",
    "c) _________\n",
    "\n",
    "d) _________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'@context': ['http://api.conceptnet.io/ld/conceptnet5.7/context.ld.json'],\n",
      " '@id': '/query?node=/c/en/california/n&rel=/r/PartOf&start=/c/en/california',\n",
      " 'edges': [{'@id': '/a/[/r/PartOf/,/c/en/california/n/wn/location/,/c/en/southwest/n/wn/location/]',\n",
      "            '@type': 'Edge',\n",
      "            'dataset': '/d/wordnet/3.1',\n",
      "            'end': {'@id': '/c/en/southwest/n/wn/location',\n",
      "                    '@type': 'Node',\n",
      "                    'label': 'Southwest',\n",
      "                    'language': 'en',\n",
      "                    'sense_label': 'n, location',\n",
      "                    'term': '/c/en/southwest'},\n",
      "            'license': 'cc:by/4.0',\n",
      "            'rel': {'@id': '/r/PartOf', '@type': 'Relation', 'label': 'PartOf'},\n",
      "            'sources': [{'@id': '/s/resource/wordnet/rdf/3.1',\n",
      "                         '@type': 'Source',\n",
      "                         'contributor': '/s/resource/wordnet/rdf/3.1'}],\n",
      "            'start': {'@id': '/c/en/california/n/wn/location',\n",
      "                      '@type': 'Node',\n",
      "                      'label': 'California',\n",
      "                      'language': 'en',\n",
      "                      'sense_label': 'n, location',\n",
      "                      'term': '/c/en/california'},\n",
      "            'surfaceText': '[[California]] is a part of [[Southwest]]',\n",
      "            'weight': 2.0},\n",
      "           {'@id': '/a/[/r/PartOf/,/c/en/california/n/wn/location/,/c/en/united_states/n/wn/location/]',\n",
      "            '@type': 'Edge',\n",
      "            'dataset': '/d/wordnet/3.1',\n",
      "            'end': {'@id': '/c/en/united_states/n/wn/location',\n",
      "                    '@type': 'Node',\n",
      "                    'label': 'United States',\n",
      "                    'language': 'en',\n",
      "                    'sense_label': 'n, location',\n",
      "                    'term': '/c/en/united_states'},\n",
      "            'license': 'cc:by/4.0',\n",
      "            'rel': {'@id': '/r/PartOf', '@type': 'Relation', 'label': 'PartOf'},\n",
      "            'sources': [{'@id': '/s/resource/wordnet/rdf/3.1',\n",
      "                         '@type': 'Source',\n",
      "                         'contributor': '/s/resource/wordnet/rdf/3.1'}],\n",
      "            'start': {'@id': '/c/en/california/n/wn/location',\n",
      "                      '@type': 'Node',\n",
      "                      'label': 'California',\n",
      "                      'language': 'en',\n",
      "                      'sense_label': 'n, location',\n",
      "                      'term': '/c/en/california'},\n",
      "            'surfaceText': '[[California]] is a part of [[United States]]',\n",
      "            'weight': 2.0}],\n",
      " 'version': '5.8.1'}\n"
     ]
    }
   ],
   "source": [
    "word = \"California\"\n",
    "word = word.lower()\n",
    "if (len(word.split())>0):\n",
    "  word = word.replace(\" \",\"_\")\n",
    "\n",
    "\n",
    "url = \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word)\n",
    "obj = requests.get(url).json()\n",
    "\n",
    "pprint.pprint (obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word:  California\n",
      "\n",
      "Distractors  ['Texas', 'Arizona', 'New Mexico', 'Nevada', 'Kansas', 'New England', 'Florida', 'Montana', 'Twin', 'Alabama', 'Yosemite', 'Connecticut', 'Mid-Atlantic states']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Distractors from http://conceptnet.io/\n",
    "def get_distractors_conceptnet(word):\n",
    "    word = word.lower()\n",
    "    original_word= word\n",
    "    if (len(word.split())>0):\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    distractor_list = [] \n",
    "    url = \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word)\n",
    "    obj = requests.get(url).json()\n",
    "\n",
    "    for edge in obj['edges']:\n",
    "        link = edge['end']['term'] \n",
    "\n",
    "        url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
    "        obj2 = requests.get(url2).json()\n",
    "        for edge in obj2['edges']:\n",
    "            word2 = edge['start']['label']\n",
    "            if word2 not in distractor_list and original_word.lower() not in word2.lower():\n",
    "                distractor_list.append(word2)\n",
    "                   \n",
    "    return distractor_list\n",
    "\n",
    "original_word = \"California\"\n",
    "distractors = get_distractors_conceptnet(original_word)\n",
    "\n",
    "print (\"Original word: \",original_word)\n",
    "print (\"\\nDistractors \",distractors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Wordvectors (sense2vec) to generate distractors\n",
    "\n",
    "https://github.com/explosion/sense2vec\n",
    "\n",
    "Download sense2vec wordvectors and unzip them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sense2vec==1.0.2 in c:\\anaconda\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: catalogue>=0.0.4 in c:\\anaconda\\lib\\site-packages (from sense2vec==1.0.2) (1.0.0)\n",
      "Requirement already satisfied: spacy<3.0.0,>=2.2.3 in c:\\anaconda\\lib\\site-packages (from sense2vec==1.0.2) (2.3.7)\n",
      "Requirement already satisfied: srsly>=0.2.0 in c:\\anaconda\\lib\\site-packages (from sense2vec==1.0.2) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\anaconda\\lib\\site-packages (from sense2vec==1.0.2) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\anaconda\\lib\\site-packages (from sense2vec==1.0.2) (1.21.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\anaconda\\lib\\site-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (1.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\anaconda\\lib\\site-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (4.64.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\anaconda\\lib\\site-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (3.0.7)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda\\lib\\site-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (61.2.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\anaconda\\lib\\site-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\anaconda\\lib\\site-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (2.27.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\anaconda\\lib\\site-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (2.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\anaconda\\lib\\site-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (0.7.8)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\anaconda\\lib\\site-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (7.4.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install sense2vec==1.0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved under s2v_reddit_2015_md.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x ./._s2v_old\n",
      "x ./s2v_old/\n",
      "x ./s2v_old/._freqs.json\n",
      "x ./s2v_old/freqs.json\n",
      "x ./s2v_old/._vectors\n",
      "x ./s2v_old/vectors\n",
      "x ./s2v_old/._cfg\n",
      "x ./s2v_old/cfg\n",
      "x ./s2v_old/._strings.json\n",
      "x ./s2v_old/strings.json\n",
      "x ./s2v_old/._key2row\n",
      "x ./s2v_old/key2row\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is EC6E-A972\n",
      "\n",
      " Directory of c:\\Users\\Afrin Peshimam\\Downloads\\AFRIN\\NLP\\Question Generation\\s2v_old\n",
      "\n",
      "09/28/2019  04:26 PM    <DIR>          .\n",
      "09/04/2022  02:10 PM    <DIR>          ..\n",
      "11/18/2019  04:04 AM               174 ._cfg\n",
      "09/28/2019  04:26 PM               174 ._freqs.json\n",
      "09/28/2019  04:26 PM               174 ._key2row\n",
      "09/28/2019  04:26 PM               174 ._strings.json\n",
      "09/28/2019  04:26 PM               174 ._vectors\n",
      "11/18/2019  04:04 AM               424 cfg\n",
      "09/28/2019  04:26 PM        49,969,681 freqs.json\n",
      "09/28/2019  04:26 PM        16,492,891 key2row\n",
      "09/28/2019  04:26 PM        26,188,439 strings.json\n",
      "09/28/2019  04:26 PM       611,973,760 vectors\n",
      "              10 File(s)    704,626,065 bytes\n",
      "               2 Dir(s)  35,720,794,112 bytes free\n"
     ]
    }
   ],
   "source": [
    "!python -m wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
    "!tar -xvf  s2v_reddit_2015_md.tar.gz\n",
    "!dir s2v_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who is the 45th president of the United States?\n",
    "\n",
    "a) ___________\n",
    "\n",
    "b) ___________\n",
    "\n",
    "c) Donald Trump\n",
    "\n",
    "d) ___________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word  donald_trump\n",
      "Best sense  Donald_Trump|PERSON\n",
      "[('Sarah_Palin|PERSON', 0.8547), ('Mitt_Romney|PERSON', 0.8246), ('Barrack_Obama|PERSON', 0.8082), ('Bill_Clinton|PERSON', 0.8046), ('Oprah|GPE', 0.8042), ('Paris_Hilton|ORG', 0.7963), ('Palin|GPE', 0.7953), ('Oprah_Winfrey|PERSON', 0.7941), ('Stephen_Colbert|PERSON', 0.7927), ('Oprah|PERSON', 0.79), ('Hilary_Clinton|PERSON', 0.7896), ('Herman_Cain|PERSON', 0.787)]\n"
     ]
    }
   ],
   "source": [
    "# load sense2vec vectors\n",
    "from sense2vec import Sense2Vec\n",
    "s2v = Sense2Vec().from_disk('s2v_old')\n",
    "\n",
    "word = \"Donald Trump\"\n",
    "word = word.lower()\n",
    "word = word.replace(\" \", \"_\")\n",
    "\n",
    "print (\"word \",word)\n",
    "\n",
    "sense = s2v.get_best_sense(word)\n",
    "\n",
    "print (\"Best sense \",sense)\n",
    "most_similar = s2v.most_similar(sense, n=12)\n",
    "\n",
    "\n",
    "print (most_similar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sarah Palin', 'Mitt Romney', 'Barrack Obama', 'Bill Clinton', 'Oprah', 'Paris Hilton', 'Palin', 'Oprah Winfrey', 'Stephen Colbert', 'Oprah', 'Hilary Clinton', 'Herman Cain']\n"
     ]
    }
   ],
   "source": [
    "distractors = []\n",
    "\n",
    "for each_word in most_similar:\n",
    "  append_word = each_word[0].split(\"|\")[0].replace(\"_\", \" \").lower()\n",
    "  if append_word.lower() != word:\n",
    "      distractors.append(append_word.title())\n",
    "\n",
    "print (distractors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distractors for  Natural Language processing  : \n",
      "['Machine Learning', 'Computer Vision', 'Deep Learning', 'Data Analysis', 'Neural Nets', 'Relational Databases', 'Algorithms', 'Neural Networks', 'Data Processing', 'Image Recognition', 'Nlp', 'Big Data', 'Data Science', 'Big Data Analysis', 'Information Retrieval', 'Speech Recognition', 'Programming Languages']\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "def sense2vec_get_words(word,s2v):\n",
    "    output = []\n",
    "    word = word.lower()\n",
    "    word = word.replace(\" \", \"_\")\n",
    "\n",
    "    sense = s2v.get_best_sense(word)\n",
    "    most_similar = s2v.most_similar(sense, n=20)\n",
    "\n",
    "    # print (\"most_similar \",most_similar)\n",
    "\n",
    "    for each_word in most_similar:\n",
    "        append_word = each_word[0].split(\"|\")[0].replace(\"_\", \" \").lower()\n",
    "        if append_word.lower().replace(' ','_') != word:\n",
    "            output.append(append_word.title())\n",
    "\n",
    "    out = list(OrderedDict.fromkeys(output))\n",
    "    return out\n",
    "\n",
    "word = \"Natural Language processing\"\n",
    "distractors = sense2vec_get_words(word,s2v)\n",
    "\n",
    "print (\"Distractors for \",word, \" : \")\n",
    "print (distractors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distractors for  USA  : \n",
      "['Usa.', 'U.S', 'U.S.', 'Us.', 'Us', 'America', 'Canada', 'U.S.A', 'United States', 'Country', 'Only Country', 'Mexico', 'Other Countries', 'U.K.', 'Europe', 'U.S.A.']\n"
     ]
    }
   ],
   "source": [
    "word = \"USA\"\n",
    "distractors = sense2vec_get_words(word,s2v)\n",
    "\n",
    "print (\"Distractors for \",word, \" : \")\n",
    "print (distractors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarity.normalized_levenshtein import NormalizedLevenshtein\n",
    "normalized_levenshtein = NormalizedLevenshtein()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance USA  & U.S.A  -> 0.4\n",
      "Levenshtein Distance USA  & U.S  -> 0.6666666666666666\n",
      "Levenshtein Distance USA  & America  -> 1.0\n",
      "Levenshtein Distance USA  & Canada  -> 1.0\n",
      "Levenshtein Distance USA  & United States  -> 0.8461538461538461\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (\"Levenshtein Distance USA  & U.S.A  ->\", normalized_levenshtein.distance(\"USA\",\"U.S.A\"))\n",
    "print (\"Levenshtein Distance USA  & U.S  ->\", normalized_levenshtein.distance(\"USA\",\"U.S\"))\n",
    "print (\"Levenshtein Distance USA  & America  ->\", normalized_levenshtein.distance(\"USA\",\"America\"))\n",
    "print (\"Levenshtein Distance USA  & Canada  ->\", normalized_levenshtein.distance(\"USA\",\"Canada\"))\n",
    "print (\"Levenshtein Distance USA  & United States  ->\", normalized_levenshtein.distance(\"USA\",\"United States\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['America', 'Canada', 'United States', 'Country', 'Only Country', 'Mexico', 'Other Countries', 'U.K.', 'Europe']]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.7\n",
    "filtered_distractors_edit_distance_and_levenshtein_distance =[[x for x in distractors if normalized_levenshtein.distance(x.lower(),word.lower())>threshold] ]\n",
    "print (filtered_distractors_edit_distance_and_levenshtein_distance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f5583cf1d9466b5c27e75c89cc6b383bed5736d6b16c51c8074d8690011a952"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
